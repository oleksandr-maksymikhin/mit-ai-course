{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oleksandr-maksymikhin/mit-ai-course/blob/main/AI_2_Assignment_2_Prediction_with_LogReg_%26_CART_%26_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting loan repayment\n",
        "In the lending industry, investors provide loans to borrowers in exchange for the promise of repayment with interest. If the borrower repays the loan, then the lender profits from the interest. However, if the borrower is unable to repay the loan, then the lender loses money. Therefore, lenders face the problem of predicting the risk of a borrower being unable to repay a loan.\n",
        "\n",
        "To address this problem, we will use publicly available data from LendingClub.com, a website that connects borrowers and investors over the Internet. This dataset represents 9,578 3-year loans that were funded through the LendingClub.com platform between May 2007 and February 2010. The binary dependent variable not_fully_paid indicates that the loan was not paid back in full (the borrower either defaulted or the loan was \"charged off,\" meaning the borrower was deemed unlikely to ever pay it back).\n",
        "\n",
        "To predict this dependent variable, we will use the following independent variables available to the investor when deciding whether to fund a loan:\n",
        "\n",
        "\n",
        "*   **credit.policy:** 1 if the customer meets the credit underwriting criteria of LendingClub.com, and 0 otherwise.\n",
        "*   **purpose:** The purpose of the loan (takes values \"credit_card\", \"debt_consolidation\", \"educational\", \"major_purchase\", \"small_business\", and \"all_other\").\n",
        "*   **int.rate:** The interest rate of the loan, as a proportion (a rate of 11% would be stored as 0.11). Borrowers judged by LendingClub.com to be more risky are assigned higher interest rates.\n",
        "*   **installment:** The monthly installments ($) owed by the borrower if the loan is funded.\n",
        "*   **log.annual.inc:** The natural log of the self-reported annual income of the borrower.\n",
        "*   **dti:** The debt-to-income ratio of the borrower (amount of debt divided by annual income).\n",
        "*   **fico:** The FICO credit score of the borrower.\n",
        "*   **days.with.cr.line:** The number of days the borrower has had a credit line.\n",
        "*   **revol.bal:** The borrower's revolving balance (amount unpaid at the end of the credit card billing cycle).\n",
        "*   **revol.util:** The borrower's revolving line utilization rate (the amount of the credit line used relative to total credit available).\n",
        "*   **inq.last.6mths:** The borrower's number of inquiries by creditors in the last 6 months.\n",
        "*   **delinq.2yrs:** The number of times the borrower had been 30+ days past due on a payment in the past 2 years.\n",
        "*   **pub.rec:** The borrower's number of derogatory public records (bankruptcy filings, tax liens, or judgments)."
      ],
      "metadata": {
        "id": "yWsIW_inFyDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.1 - Preparing the Dataset\n",
        "\n",
        "Load the dataset loans_imputed.csv into a data frame called loans, and explore it using the info() and describe() functions. (A side note: the original data had missing values, so we imputed them first.)\n",
        "\n",
        "What proportion of the loans in the dataset were not paid in full? Please input a number between 0 and 1.\n"
      ],
      "metadata": {
        "id": "dKM3uoykGSbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve\n",
        "import statsmodels.api as sm"
      ],
      "metadata": {
        "id": "bojQ9jK5Gwsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "loans = pd.read_csv(\"loans_imputed.csv\")"
      ],
      "metadata": {
        "id": "P7PwIKUbG9FZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 1.1\n",
        "print(loans.info())\n",
        "print(loans.describe())"
      ],
      "metadata": {
        "id": "mOzp3vEqHCRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66f368e3-7e77-4c1f-966e-5fd06a090d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 9578 entries, 0 to 9577\n",
            "Data columns (total 14 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   credit.policy      9578 non-null   int64  \n",
            " 1   purpose            9578 non-null   object \n",
            " 2   int.rate           9578 non-null   float64\n",
            " 3   installment        9578 non-null   float64\n",
            " 4   log.annual.inc     9578 non-null   float64\n",
            " 5   dti                9578 non-null   float64\n",
            " 6   fico               9578 non-null   int64  \n",
            " 7   days.with.cr.line  9578 non-null   float64\n",
            " 8   revol.bal          9578 non-null   int64  \n",
            " 9   revol.util         9578 non-null   float64\n",
            " 10  inq.last.6mths     9578 non-null   int64  \n",
            " 11  delinq.2yrs        9578 non-null   int64  \n",
            " 12  pub.rec            9578 non-null   int64  \n",
            " 13  not.fully.paid     9578 non-null   int64  \n",
            "dtypes: float64(6), int64(7), object(1)\n",
            "memory usage: 1.0+ MB\n",
            "None\n",
            "       credit.policy     int.rate  installment  log.annual.inc          dti  \\\n",
            "count    9578.000000  9578.000000  9578.000000     9578.000000  9578.000000   \n",
            "mean        0.804970     0.122640   319.089413       10.932117    12.606679   \n",
            "std         0.396245     0.026847   207.071301        0.614813     6.883970   \n",
            "min         0.000000     0.060000    15.670000        7.547502     0.000000   \n",
            "25%         1.000000     0.103900   163.770000       10.558414     7.212500   \n",
            "50%         1.000000     0.122100   268.950000       10.928884    12.665000   \n",
            "75%         1.000000     0.140700   432.762500       11.291293    17.950000   \n",
            "max         1.000000     0.216400   940.140000       14.528354    29.960000   \n",
            "\n",
            "              fico  days.with.cr.line     revol.bal   revol.util  \\\n",
            "count  9578.000000        9578.000000  9.578000e+03  9578.000000   \n",
            "mean    710.846314        4560.767197  1.691396e+04    46.799236   \n",
            "std      37.970537        2496.930377  3.375619e+04    29.014417   \n",
            "min     612.000000         178.958333  0.000000e+00     0.000000   \n",
            "25%     682.000000        2820.000000  3.187000e+03    22.600000   \n",
            "50%     707.000000        4139.958333  8.596000e+03    46.300000   \n",
            "75%     737.000000        5730.000000  1.824950e+04    70.900000   \n",
            "max     827.000000       17639.958330  1.207359e+06   119.000000   \n",
            "\n",
            "       inq.last.6mths  delinq.2yrs      pub.rec  not.fully.paid  \n",
            "count     9578.000000  9578.000000  9578.000000     9578.000000  \n",
            "mean         1.577469     0.163708     0.062122        0.160054  \n",
            "std          2.200245     0.546215     0.262126        0.366676  \n",
            "min          0.000000     0.000000     0.000000        0.000000  \n",
            "25%          0.000000     0.000000     0.000000        0.000000  \n",
            "50%          1.000000     0.000000     0.000000        0.000000  \n",
            "75%          2.000000     0.000000     0.000000        0.000000  \n",
            "max         33.000000    13.000000     5.000000        1.000000  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loans[\"not.fully.paid\"].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "Wu1POoaZHSc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a1f701f-8e44-4f31-c8fa-63827831417a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not.fully.paid\n",
            "0    0.839946\n",
            "1    0.160054\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.1 - Prediction Models\n",
        "\n",
        "Now that we have explored the dataset, we need to split it into a training and testing set. To ensure everybody obtains the same split, set the random seed to 144 and use the train_test_split function to select the 70% of observations for the training set (the dependent variable for train_test_split is not.fully.paid). Name the data frames train and test.\n",
        "\n",
        "Now, use logistic regression trained on the training set to predict the dependent variable not.fully.paid using all the independent variables.\n",
        "\n",
        "Which independent variables are significant in our model? (Significant variables have at least one star, or a Pr(>|z|) value less than 0.05.) Select all that apply.\n",
        "*   credit.policy\n",
        "*   purpose2 (credit card)\n",
        "*   purpose3 (debt consolidation)\n",
        "*   purpose4 (educational)\n",
        "*   purpose5 (home improvement)\n",
        "*   purpose6 (major purchase)\n",
        "*   purpose7 (small business)\n",
        "*   int.rate\n",
        "*   installment\n",
        "*   log.annual.inc\n",
        "*   dti\n",
        "*   fico\n",
        "*   days.with.cr.line\n",
        "*   revol.bal\n",
        "*   revol.util\n",
        "*   inq.last.6mths\n",
        "*   delinq.2yrs\n",
        "*   pub.rec\n",
        "\n"
      ],
      "metadata": {
        "id": "2CZvG0qxHTw0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bwT-BsE8Fq-Z"
      },
      "outputs": [],
      "source": [
        "# Question 2.1\n",
        "np.random.seed(144)\n",
        "train, test = train_test_split(loans, test_size=0.3, stratify=loans[\"not.fully.paid\"], random_state=144)\n",
        "\n",
        "# Fit logistic regression model\n",
        "X_train = train.drop(columns=[\"not.fully.paid\"])\n",
        "y_train = train[\"not.fully.paid\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = pd.get_dummies(X_train, columns=['purpose'], drop_first=True, dtype=int)"
      ],
      "metadata": {
        "id": "sec2GoUTHqgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = sm.Logit(y_train, sm.add_constant(X_train)).fit()\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "3c2_ZiXwHoDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51105fa2-3bd3-4cae-aa3e-4169cdd222d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.408299\n",
            "         Iterations 6\n",
            "                           Logit Regression Results                           \n",
            "==============================================================================\n",
            "Dep. Variable:         not.fully.paid   No. Observations:                 6704\n",
            "Model:                          Logit   Df Residuals:                     6685\n",
            "Method:                           MLE   Df Model:                           18\n",
            "Date:                Mon, 14 Jul 2025   Pseudo R-squ.:                 0.07154\n",
            "Time:                        00:48:42   Log-Likelihood:                -2737.2\n",
            "converged:                       True   LL-Null:                       -2948.1\n",
            "Covariance Type:            nonrobust   LLR p-value:                 2.563e-78\n",
            "==============================================================================================\n",
            "                                 coef    std err          z      P>|z|      [0.025      0.975]\n",
            "----------------------------------------------------------------------------------------------\n",
            "const                          8.9154      1.542      5.782      0.000       5.893      11.937\n",
            "credit.policy                 -0.2319      0.101     -2.286      0.022      -0.431      -0.033\n",
            "int.rate                       1.8495      2.086      0.887      0.375      -2.239       5.938\n",
            "installment                    0.0012      0.000      5.940      0.000       0.001       0.002\n",
            "log.annual.inc                -0.4337      0.071     -6.101      0.000      -0.573      -0.294\n",
            "dti                           -0.0025      0.005     -0.449      0.653      -0.013       0.008\n",
            "fico                          -0.0092      0.002     -5.414      0.000      -0.013      -0.006\n",
            "days.with.cr.line           2.023e-06   1.61e-05      0.126      0.900   -2.95e-05    3.36e-05\n",
            "revol.bal                   3.941e-06   1.16e-06      3.399      0.001    1.67e-06    6.21e-06\n",
            "revol.util                     0.0018      0.002      1.179      0.238      -0.001       0.005\n",
            "inq.last.6mths                 0.1010      0.017      6.040      0.000       0.068       0.134\n",
            "delinq.2yrs                   -0.0989      0.068     -1.463      0.143      -0.231       0.034\n",
            "pub.rec                        0.3764      0.117      3.207      0.001       0.146       0.607\n",
            "purpose_credit_card           -0.5842      0.134     -4.366      0.000      -0.846      -0.322\n",
            "purpose_debt_consolidation    -0.3143      0.093     -3.392      0.001      -0.496      -0.133\n",
            "purpose_educational           -0.1153      0.192     -0.600      0.548      -0.492       0.261\n",
            "purpose_home_improvement       0.0866      0.156      0.555      0.579      -0.219       0.393\n",
            "purpose_major_purchase        -0.2624      0.190     -1.383      0.167      -0.634       0.109\n",
            "purpose_small_business         0.5974      0.136      4.407      0.000       0.332       0.863\n",
            "==============================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.2 - Prediction Models\n",
        "\n",
        "Consider two loan applications, which are identical other than the fact that the borrower in Application A has FICO credit score 700 while the borrower in Application B has FICO credit score 710.\n",
        "\n",
        "Let Logit(A) be the log odds of loan A not being paid back in full, according to our logistic regression model, and define Logit(B) similarly for loan B. What is the value of Logit(A) - Logit(B)?"
      ],
      "metadata": {
        "id": "RZTkfjxuHomN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let O(A) be the odds of loan A not being paid back in full, according to our logistic regression model, and define O(B) similarly for loan B. What is the value of O(A)/O(B)? (HINT: Use the mathematical rule that math.exp(A + B + C) = math.exp(A)\\*math.exp(B)\\*math.exp(C), where math is a Python library that we have imported into the Colab. Also, remember that math.exp() is the exponential function in Python.)"
      ],
      "metadata": {
        "id": "urqUfZ8QHooV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.3 - Prediction Models\n",
        "\n",
        "Predict the probability of the test set loans not being paid back in full (remember type=\"response\" for the predict function). Store these predicted probabilities in a variable named predicted.risk and add it to your test set (we will use this variable in later parts of the problem). Compute the confusion matrix using a threshold of 0.5.\n",
        "\n",
        "What is the accuracy of the logistic regression model? Input the accuracy as a number between 0 and 1."
      ],
      "metadata": {
        "id": "wF3nFVUGeAb8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the accuracy of the baseline model? Input the accuracy as a number between 0 and 1."
      ],
      "metadata": {
        "id": "gwkIZy15eCJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2.3\n",
        "X_test = test.drop(columns=[\"not.fully.paid\"])\n",
        "y_test = test[\"not.fully.paid\"]\n",
        "X_test = pd.get_dummies(X_test, columns=['purpose'], drop_first=True, dtype=int)\n",
        "test[\"predicted.risk\"] = model.predict(sm.add_constant(X_test))\n",
        "\n",
        "conf = confusion_matrix(y_test, test[\"predicted.risk\"] > 0.5)\n",
        "print(conf)\n",
        "print(conf.diagonal().sum() / conf.sum())"
      ],
      "metadata": {
        "id": "ZVgkYTKneLld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[\"not.fully.paid\"].value_counts())\n",
        "print(5631 / (1073 + 5631))\n",
        "print(test[\"not.fully.paid\"].value_counts())\n",
        "print(2414 / (2414 + 460))"
      ],
      "metadata": {
        "id": "lPEdNu2peVSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.4 - Prediction Models\n",
        "\n",
        "Compute the test set AUC. The model has poor accuracy at the threshold 0.5. But despite the poor accuracy, we will see later how an investor can still leverage this logistic regression model to make profitable investments."
      ],
      "metadata": {
        "id": "WBadgHWyeg59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 2.4\n",
        "logRegAUC = roc_auc_score(y_test, test[\"predicted.risk\"])\n",
        "print(\"AUC:\", logRegAUC)"
      ],
      "metadata": {
        "id": "G6WgvqOUemFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3.1 - A \"Smart Baseline\"\n",
        "\n",
        "In the previous problem, we built a logistic regression model that has an AUC significantly higher than the AUC of 0.5 that would be obtained by randomly ordering observations.\n",
        "\n",
        "However, LendingClub.com assigns the interest rate to a loan based on their estimate of that loan's risk. This variable, int.rate, is an independent variable in our dataset. In this part, we will investigate using the loan's interest rate as a \"smart baseline\" to order the loans according to risk.\n",
        "\n",
        "Using the training set, build a bivariate logistic regression model (aka a logistic regression model with a single independent variable) that predicts the dependent variable not.fully.paid using only the variable int.rate.\n",
        "\n",
        "The variable int.rate is highly significant in the bivariate model, but it is not significant at the 0.05 level in the model trained with all the independent variables. What is the most likely explanation for this difference?\n",
        "\n",
        "\n",
        "\n",
        "*   int.rate is correlated with other risk-related variables, and therefore does not incrementally improve the model when those other variables are included.\n",
        "*   This effect is likely due to the training/testing set split we used. In other splits, we could see the opposite effect.\n",
        "*   These models are trained on a different set of observations, so the coefficients are not comparable.\n",
        "\n"
      ],
      "metadata": {
        "id": "grqDv3M1ewEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3.1\n",
        "mod2 = sm.Logit(y_train, sm.add_constant(train[[\"int.rate\"]])).fit()\n",
        "print(mod2.summary())"
      ],
      "metadata": {
        "id": "MZrr2J26e-wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['int.rate'].corr(train['fico'])"
      ],
      "metadata": {
        "id": "iXn17jR-fdXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3.2 - A \"Smart Baseline\"\n",
        "\n",
        "Make test set predictions for the bivariate model. What is the highest predicted probability of a loan not being paid in full on the testing set?"
      ],
      "metadata": {
        "id": "TywnFUpTf-g1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3.2\n",
        "pred2 = mod2.predict(sm.add_constant(test[[\"int.rate\"]]))\n",
        "print(confusion_matrix(y_test, pred2 > 0.5))\n",
        "print(np.max(pred2))  # Tail of sorted predictions"
      ],
      "metadata": {
        "id": "vezXh6MagCeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3.3 - A \"Smart Baseline\"\n",
        "\n",
        "What is the test set AUC of the bivariate model?"
      ],
      "metadata": {
        "id": "H7yG1KTbgO9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 3.3\n",
        "logRegAUC2 = roc_auc_score(y_test, pred2)\n",
        "print(\"AUC2:\", logRegAUC2)"
      ],
      "metadata": {
        "id": "ck6QDXyJHg3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting Stock Returns with Tree-based Methods\n",
        "\n",
        "In this assignment, we'll use Tree-based Methods to predict future stock prices using historical stock data.\n",
        "\n",
        "When selecting which stocks to invest in, investors seek to obtain good future returns. In this problem, we'll use logistic regression and tree-based methods to predict whether or not the stocks will have positive future returns.\n",
        "\n",
        "For this problem, we'll use StocksCluster.csv, which contains monthly stock returns from the NASDAQ stock exchange. The NASDAQ is the second-largest stock exchange in the world, and it lists many technology companies. The stock price data used in this problem was obtained from infochimps, a website providing access to many datasets.\n",
        "\n",
        "Each observation in the dataset is the monthly returns of a particular company in a particular year. The years included are 2000-2009. The companies are limited to tickers that were listed on the exchange for the entire period 2000-2009, and whose stock price never fell below $1. So, for example, one observation is for Yahoo in 2000, and another observation is for Yahoo in 2001. Our goal will be to predict whether or not the stock return in December will be positive, using the stock returns for the first 11 months of the year.\n",
        "\n",
        "This dataset contains the following variables:\n",
        "\n",
        "\n",
        "*   **ReturnJan** = the return for the company's stock during January (in the year of the observation). The rest follows the same format.\n",
        "*   **PositiveDec** = whether or not the company's stock had a positive return in December (in the year of the observation). This variable takes value 1 if the return was positive, and value 0 if the return was not positive.\n",
        "\n",
        "For the first 11 variables, the value stored is a proportional change in stock value during that month. For instance, a value of 0.05 means the stock increased in value 5% during the month, while a value of -0.02 means the stock decreased in value 2% during the month."
      ],
      "metadata": {
        "id": "Q-aLp3Rsga6x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.1 - Exploring the Dataset\n",
        "\n",
        "Load StocksCluster.csv into a data frame called \"stocks\". How many observations are in the dataset?"
      ],
      "metadata": {
        "id": "gV_dD_bPga9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Load Data\n",
        "stocks = pd.read_csv(\"StocksCluster.csv\")"
      ],
      "metadata": {
        "id": "YRx4MfoIhy4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stocks)"
      ],
      "metadata": {
        "id": "y3Yk_bH1h2T_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.2 - Exploring the Dataset\n",
        "\n",
        "What proportion of the observations have positive returns in December?"
      ],
      "metadata": {
        "id": "k6L5pYetgbDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks[\"PositiveDec\"].mean()"
      ],
      "metadata": {
        "id": "riA8G4xyh7P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.3 - Exploring the Dataset\n",
        "\n",
        "What is the maximum correlation between any two return variables in the dataset? You should look at the pairwise correlations between ReturnJan, ReturnFeb, ReturnMar, ReturnApr, ReturnMay, ReturnJune, ReturnJuly, ReturnAug, ReturnSep, ReturnOct, and ReturnNov."
      ],
      "metadata": {
        "id": "_H3Zm9lniETz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correlation_matrix = stocks.corr()\n",
        "correlation_matrix[correlation_matrix != 1].max().max()"
      ],
      "metadata": {
        "id": "BB63vaDYh8lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 1.4 - Exploring the Dataset\n",
        "\n",
        "\n",
        "Which month (from January through November) has the largest mean return across all observations in the dataset?\n",
        "\n",
        "Which month (from January through November) has the smallest mean return across all observations in the dataset?"
      ],
      "metadata": {
        "id": "akXzf9C4j1Zl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks.mean()"
      ],
      "metadata": {
        "id": "RvRbQR7mjj5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.1 - Initial Logistic Regression Model\n",
        "\n",
        "Split the data into a training set and testing set, putting 70% of the data in the training set and 30% of the data in the testing set. Then, use the Train data frame to train a logistic regression model (name it StocksModel) to predict PositiveDec using all the other variables as independent variables.\n",
        "\n",
        "What is the overall accuracy on the training set, using a threshold of 0.5?"
      ],
      "metadata": {
        "id": "ijTXITCNkQfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "np.random.seed(144)\n",
        "X = stocks.drop(columns=[\"PositiveDec\"])\n",
        "y = stocks[\"PositiveDec\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=144)\n",
        "\n",
        "# Logistic Regression Model\n",
        "X_train_const = sm.add_constant(X_train)\n",
        "X_test_const = sm.add_constant(X_test)\n",
        "logit_model = sm.Logit(y_train, X_train_const).fit()"
      ],
      "metadata": {
        "id": "EUHay75MkDrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "log_reg_pred_prob = logit_model.predict(X_train_const)\n",
        "log_reg_pred = log_reg_pred_prob > 0.5\n",
        "\n",
        "# Confusion Matrix and Accuracy\n",
        "log_reg_conf_matrix = confusion_matrix(y_train, log_reg_pred)\n",
        "log_reg_accuracy = accuracy_score(y_train, log_reg_pred)\n",
        "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)"
      ],
      "metadata": {
        "id": "XCW7hXdilFDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.2 - Initial Logistic Regression Model\n",
        "\n",
        "Now obtain test set predictions from StocksModel. What is the overall accuracy of the model on the test, again using a threshold of 0.5?"
      ],
      "metadata": {
        "id": "Sde-IOhkkz5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predictions\n",
        "log_reg_pred_prob = logit_model.predict(X_test_const)\n",
        "log_reg_pred = log_reg_pred_prob > 0.5\n",
        "\n",
        "# Confusion Matrix and Accuracy\n",
        "log_reg_conf_matrix = confusion_matrix(y_test, log_reg_pred)\n",
        "log_reg_accuracy = accuracy_score(y_test, log_reg_pred)\n",
        "print(\"Logistic Regression Accuracy:\", log_reg_accuracy)"
      ],
      "metadata": {
        "id": "Z8M2bpIAkdvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 2.3 - Initial Logistic Regression Model\n",
        "\n",
        "What is the accuracy on the test set of a baseline model that always predicts the most common outcome (PositiveDec = 1)?"
      ],
      "metadata": {
        "id": "RH24-A6ulSaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(y_test == 1).mean()"
      ],
      "metadata": {
        "id": "9ZSk3dKylEIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3.1 - CART Model\n",
        "\n",
        "Then, use the stocksTrain data frame to train a CART model to predict PositiveDec using all the other variables as independent variables.\n",
        "\n",
        "What is the AUC on the test set?"
      ],
      "metadata": {
        "id": "rq9l25g1lqfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Model\n",
        "cart_model = DecisionTreeClassifier(random_state=144)\n",
        "cart_model.fit(X_train, y_train)\n",
        "cart_pred_prob = cart_model.predict_proba(X_test)[:, 1]\n",
        "cart_pred = cart_pred_prob > 0.5\n",
        "\n",
        "# Confusion Matrix and Accuracy\n",
        "cart_conf_matrix = confusion_matrix(y_test, cart_pred)\n",
        "cart_accuracy = accuracy_score(y_test, cart_pred)\n",
        "print(\"Decision Tree Accuracy:\", cart_accuracy)\n",
        "\n",
        "# Decision Tree AUC\n",
        "cart_auc = roc_auc_score(y_test, cart_pred_prob)\n",
        "print(\"Decision Tree AUC:\", cart_auc)\n"
      ],
      "metadata": {
        "id": "gO6qp-_Alb75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 3.2 - CART Model\n",
        "\n",
        "Visualize the CART tree plot you just build. Which month (from January through November) does the CART model first split on?"
      ],
      "metadata": {
        "id": "52KqmUQ7l3xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Decision Tree\n",
        "from sklearn.tree import plot_tree\n",
        "plt.figure(figsize=(12,8))\n",
        "plot_tree(cart_model, filled=True, feature_names=X.columns, class_names=[\"0\", \"1\"], max_depth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "J2fXbUCulxJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 4.1 - Random Forest Model\n",
        "\n",
        "Then, use the stocksTrain data frame to train a Random Forest model to predict PositiveDec using all the other variables as independent variables. Please set the random seed to 144, and set the random forest model parameters to n_estimators=1000, min_samples_leaf=10.\n",
        "\n",
        "What is the AUC on the test set?"
      ],
      "metadata": {
        "id": "jKklKJ8jmDXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "\n",
        "# Random Forest Model\n",
        "rf_model = RandomForestClassifier(n_estimators=1000, min_samples_leaf=10, random_state=144)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred_prob = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Random Forest AUC\n",
        "rf_auc = roc_auc_score(y_test, rf_pred_prob)\n",
        "print(\"Random Forest AUC:\", rf_auc)"
      ],
      "metadata": {
        "id": "_TbWeBfZnjRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem 5.1 - Boosting Trees\n",
        "\n",
        "Lastly, we will use a Boosting Tree model to predict PositiveDec using all the other variables as independent variables. Please set the random seed to 144, and set the model parameters to n_estimators = 1000, learning_rate = 0.001, max_depth = 10.\n",
        "\n",
        "What is the AUC on the test set?"
      ],
      "metadata": {
        "id": "BodHM1DXmFvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting Model\n",
        "gbm_model = GradientBoostingClassifier(n_estimators=1000, learning_rate=0.001,\n",
        "                                       max_depth=10, random_state=144)\n",
        "gbm_model.fit(X_train, y_train)\n",
        "gbm_pred_prob = gbm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Gradient Boosting AUC\n",
        "gbm_auc = roc_auc_score(y_test, gbm_pred_prob)\n",
        "print(\"Gradient Boosting AUC:\", gbm_auc)"
      ],
      "metadata": {
        "id": "tiH98YiDl9Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vlOoiXFln6wU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}